# Simple Predictive Coding Networks for Motion Prediction
### By Nick Alonso

Engineers building artificial neural networks for vision tasks typically utilize feedforward neural networks. Cortical regions of the human brain, especially the visual cortex, however, utilize large amounts of feedback connections. There is good reason, then, to believe that neural networks used for vision tasks may be improved and reach more human-like performance if they utilized feedback connections. The challenge, however, is in figuring out how to integrate feedback connections into artificial neural networks, how to train them efficientaly, and what they should be trained to do. 
  
One theory that may provide significant insight into these questions is the theory of predictive coding. This theory states that the large number of feedback connections found in cortical regions of the brain are used, mainly, to predict neural activity at lower cortical regions (Spratling, 2017). Feedforward connections are tasked with propagating forward an error signal: a measure of the difference between predicted neural activity and actual neural activity. This architecture allows the brain to encode information efficientaly, as only information about unpredicted aspects of neural activity is propogated forward rather than information about all of it (Huang and Ballard, 2011). This theory was developed and modeled, most notabely, by Rao and Ballard (1999). They built a hierarchy of small autoencoder-like neural networks. The encoder of each unit of the hierarchy took in error signals as input and outputted a compressed representation of the input (r), then r was inputted into the decoder which attempted to generate (i.e. predict) the activity of a lower level unit at the next timestep. Each unit, specifically, attempted to predict r in a subset of the units at lower levels in the hierarchy (more details below). 
  
  My goal for this project was to 1. develop a better understanding of how predictive coding models work (with a special focus on Rao and Ballard (1999)) and 2. to implement some simple predictive coding networks using pytorch. My focus was to learn and implement a working predictive coding architecture rather than to build a network for the purpose of meeting or exceeding some performance benchmark. For this reason, I decided to build a larger number of small neural networks that trained quickly on somewhat simple data, rather than one or a few large networks on more complex data. This allowed me to experiment with various predictive coding architectures quickly, and understand how they worked and what might improve (or worsen) their performance.
  
  In this paper, I present some of the findings of this project. I divide my findings into two parts. In the first part, I compare three simple predictive coding units I built (PCUs): one with linear layers, one with convolutional layers, and a variational autoencoder. In the second part, I describe how I combined five simple linear PCUs into a hierarchy, and I compare the performance of this hierarchy to PCUs that are not hierarchical but are of similar size.
  
## 1. Data: Video Generation
  Rao and Ballard (1999) built a predictive coding unit to generate an image, but the same unit could also be used to predict video. In the case of video prediction, the unit would not simply be trained to generate (at time t1) the same image presented at a prior timestep t0), but rather to predict what new image of the video will appear (at time t1) given the image that appeared at a prior timestep (time t0). In order to predict video, the unit has to learn regularities in the way images change over time rather than just the spatial properties of a single image. Predictive coding inspired neural networks have been applied by others for the purpose of video prediction with some success (e.g. Lotter et al., 2016; Piekniewski et al., 2016).
  
  I decided to use video as my input for my networks. My neural networks are small, so the videos used as input could not be too complex. I decided to generate my own simple videos. I created videos of black rectangles moving across a white background. The size of the background could be adjusted, but I used a small 30x30 frame for all of my tests. Each rectangle was composed of roughly the same number of pixels (12 to 16), but had one of five different shapes: 4x4, 3x5, 5x3, 2x8, 8x2. Each rectangle moved either straight up or down, straight left or right, or at a 45 degree angle up-left, up-right, down-left, or down-right. They also moved at various speeds (usually 1-4 pixels per frame). Thus, the neural networks, in order to generate the videos accurately, had to learn to encode information about shape and motion (i.e. direction and speed).
  
  Each video started with a rectangle positioned randomly within the frame, then proceeded to move until it left the screen. Each video was a 4D pytorch tensor, (batch, frameHeight, frameWidth, frameNumber). These were loaded into a dictionary. Using a dictionary allowed me to store videos of varying length in the same data structure. This would not be possible if I attempted to load all the videos into a tensor. I did pad the videos and load them into a tensor, but it affected the results (likely because many videos were quite short and needed a lot of padding). So I decided to not use padding, but instead store the videos in a dictionary without padding.

## 3. Part I: Three Simple Predictive Coding Units

  The first part of my project consisted of building and experimenting with single predictive coding units (PCUs) with various kinds of layers, activation functions, and size. I present three of these units here. Each predictive coding unit can be seen as special kind of autoencoder. It is important, first, to keep in mind the predictive coding unit is not just generating an image, but is predicting a future image in a video. Second, the encoder of the PCU does not take in an image as input, but rather takes in an error signal as input, which I will explain below. Third, predictive coding units can be combined into a hiearchy of units and take in input from units higher and lower in the hierarchy. I focus on building a hiearchy in the next section. Here I will only focus on building a single predictive coding unit.
  
  An error signal can be understood as follows. Let's call the image at timestep zero I<sub>0</sub> and the prediction of that image generated by PCU I<sub>Pred0</sub>. The error signal that the PCU will take as input to predict the next frame at time one is some measure the difference between I<sub>0</sub> and I<sub>Pred0</sub>. Rao and Ballard used the simple measure I<sub>0</sub> - I<sub>0</sub>, where the subtraction is an element-wise subtraction of the two matrices. The result is an error signal which is then inputted to the encoder of the PCU. Other measures of the error may be used. Spratling (2017), for example, suggests that aI<sub>0</sub> / bI<sub>0</sub> + c is more biologically plausible (a,b, and c are constants). (Though I don't show results, I found both errors acheived similar results so I stuck with Rao and Ballards simpler error computation). 

### 3.1 Building Predictive Coding Units
I experimented with many different kinds of PCUs. Here I present three simple ones. First, is a PCU that consists of three linear layers, an input layer, hidden layer, and output layer. The input and output layer are size 1x900. The hidden layer is of size 1x150. The encoder used ReLU activation functions, while the decoder used sigmoid. (The PCU would not work using only sigmoid).

Second, is a  PCU that used convolutional layers. It consisted of three layers: input, hidden layer/channels, output layer. Input and output layers were 1x30x30. A kernal of size 4 and a stride of two were ysed, with 1 to 3 output channels. Here I show the results of a PCU with an encoder that has 1 output channels. 

Third, I created a variational autoencoder. I used code found here https://github.com/pytorch/examples/blob/master/vae/main.py to build the network. This code is an implementation of Kingma and Wellington (2014). This network had an input layer of size 1x900, which fed into a hidden layer h1 of 1x400. H1 fed into two more hidden layers of size 1x120. These encoded mu and var variables (see code). Mu and var are used for reparameterization, which produces a vector of size 1x120. This is fed into the decoder which consists of three linear layers size, 1x120, 1x400, and 1x900.

Each network, it should be noted, acheives a similar, but not the same, level of compression. The linear PCU compresses the image from 1x900 to 1x150, the VAE from 1x900 to 1x120, and the convolutional PCU from 30x30 to 14x14 (=1x196).

### 3.2 Training and Results
Each network took an error signal as input. The error signal was, as noted above, I<sub>0</sub> - I<sub>Pred0</sub>. They were trained with backpropagation using an adam optimizer, which was applied every frame of the video. Here I used BCE loss to train the linear and convolutional PCU units. (Rao and Ballard (1999) use a different optimization function but I do not use that here). The loss function for the variational autoencoder was more complex, and I will refer you to the code (found in finalProjVAE.py) for more details. The convolutional and linear neural net were trained over 1500 video. The variational autoencoder took many more videos, however, to acheive a decent performance. It was trained on 750 videos of 17 epochs.

Each network performed slightly differently. Below you will see a comparison of their BCE loss over the same 500 test videos. The BCE was averaged over each video and plotted. The mean of the average BCE per video across the 500 videos is as follows for each network: Linear = .329, Convlutional = .346, VAE = .332. The standard deviations of BCE across the 500 videos were: Linear = .107, Convlutional = .12, VAE = .107.

<img src="/Data/Part I/BCEComparePartI.png">



Below you will see a side by side comparison of video frames and the predictions of those frames produced by each PCU for one video. Each network predicts random noise on the first frame. Then they all roughly generate the previous frame. One frame is not enough to know what direction and speed these shapes are moving yet, so they will need one or more frames to encode this motion information.
It becomes clear that both the VAE and the linear PCU predict motion, as they eventually stop generating the previous frame and begin predicting that the shape will be in a different position than it was last frame (i.e. that it will move). Neither, however, seem particularly good at generating the shape of the rectangle. Both generate elongated ovals with fuzzy boundaries. The convolutional PCU generates shape much better, it is less clear, however, whether it is predicting motion or just generating the previous frame with some noise. See 'Data/Part I' for more images.

<img src="/Data/Part I/PartICompare2.png">

### 3.3 Discussion
  The convolutional autoencoder generated shape far better than the VAE or the linear layer, but it was not clear from the images, if or how well it was predicting motion. This may explain why its average BCE loss over the test videos was higher than it was for the other two networks. The convolutional autoencoder also generated images with sharper boundaries around the shape than the other two neural networks did, which may have also contributed its greater BCE (large and small pixel values would generate larger errors if wrong). The VAE and the linear PCU predicted motion clearly, but were not good at predicting shape. The VAE was also at a disadvantage here, given that its hidden layer was the smallest of all three neural nets. A convolutional autoencoder could likely be made to predict motion better if more channels and layers were included.
  
  
  
## 4. PartII: Hierarchy of Predictive Coding Units
  Next, I created a hierarchy of PCUs. Rao and Ballard (1999) argued that the cortical areas of the brain can be seen as hierarchies of predictive coding units. Each unit attempts to predict the neural activity of the hidden layers/representations of PCUs at lower levels in the hierarchy. An error signal is propagated forward based on the difference between the predicted neural activity in the hidden layer and the actual neural activity. In addition to the bottom up error signal, the activity in PCU hidden layers is influenced by the top down predictions of the hidden layer made by higher-level units. Rao and Ballard (1999) implemented this top-down influence by taking the error between the neural activity of a PCU's hidden layer (r) and its top-down prediction: r - r<sub>topdown</sub>. Then they inputted this top-down error into the encoder of the PCU along with the bottom-up error. The input to a PCU's encoder is, thus, the error in its prediction of a lower unit's r and the error in the higher units prediction of its r. A PCU takes as input these error signals and outputs a compressed representation r of the lower level neural activity. 

### 4.1 Building PCU Hierarchy
  I implemented this same kind of hierarchy using five linear PCU's of the sort described in Part I. The hierarchy had two levels. The first level consisted of four PCU's, each was responsible for predicting one quarter of the image pixels. The second level consisted of one PCU that predicted the hidden layers of the four lower PCUs. The lower unit's encoder took as input the bottom up error signal (which was the difference between its prediction of 1/4 of the image pixels and the actual pixels of that image patch), and it took as input the error between its hidden layer r and the top-down prediction of r coming from the higher level unit. Because there was no third level, the second level PCU's hidden layer was not influenced by top-down predictions.
  
  The video images were 30x30. The encoders of the four lower level PCU's took as input 1x225 error signal. Their hidden layers were of size 144. Thus, the top-down error signal is of size 144 making their encoder's total input layer 1 x (225 + 144) = 1 x 369. The output layer was 1 x 225 (i.e. the size of the image patch being predicted). The second layer PCU took in all the hidder layers (the rs) of the four lower level PCUs as input. Thus, its input (and output) layer was 1 x 576. Its hidden layer was of size 1 x 300. All layers used sigmoid activation functions. No ReLUs, or any other sort of activation function was used.
  
 Each unit was trained using its own adam optimizer. A BCE loss function was used. For lower units, the loss was computed between the image patch they were predicting and their prediction of it. The second level PCU's loss was computed between the values of the hidden layers of the lower units and its prediction of those values.
 
 I compared the performance of the hierarchy to two other neural networks. Each is a single PCU that has the same number of neurons as the hierarchy does (i.e. 3828 neurons). One neural network has 5 linear layers with hidden layer of 1 x 500. The second has 7 linear layers with a hidden layer of size 1 x 300. All layers in the networks use ReLU activation functions, except for the output layers, which use sigmoid (these networks would not work, unlike the hierarchy, using only sigmoid activations). Despite have the same number of neurons, it should be noted the hierachy has fewer connections because the hidden layers of the lower PCUs and input layers because the hidden layers are not fully connected to input image. Specifically, the hidden layers at the lower levels are only connected to one fourth of the input neurons (i.e. those neurons in the same PCU) rather than all of them, which is not the case with these other two neural networks which are fully connected. The six layer PCU also has more layers than the hierachy (7 compared to 5), and the four layer PCU compresses the input less than the hierarchy: the entire image is compressed to 300 neurons in the hierarchy but only 500 in the five layer PCU. The reason for comparing these neural networks was to see if the hierarchy had any advantages in performance over neural networks of similar size. 
 
 ### 4.2 Results
 
Results can be seen below. LinearH is the hiearchy of PCUs. Linear2 is the five layer PCU and Linear3 is the seven layer PCU. The first graph shows their avg BCE loss per video during training across 300 videos (i.e. average BCE loss between entire image and prediction of full image). The rate of improvement is quite similar for the hierarchy as it is for the single prediction units. Though near the end the is some divergence in performance as they level out, which I measure below.

<img src="/Data/Part II/BCEComparison.png">
 
 
In the graph below, I tested each neural network's avg BCE loss per video after it had been trained across 500 hundred test videos. Each neural net was trained on the same 1000 training videos. Although these performances look similar, there are some important differences. First, the hierachy's performance has greater variance across videos than do the other neural networks. Second, on average, the hierarchy has lower average BCE losses than the other two neural networks. Over these 500 videos the mean of the average BCE Loss per video were as follows: LinearH = .322  Linear2 = .328 Linear3 = .332. The standard deviations were: LinearH = .116  Linear2 = .107 Linear3 = .099. These may seem like small differences, but they are consistent. Across 10 more runs of different sets of 500 videos, LinearH always had the smallest mean BCE and the largest std. 

<img src="/Data/Part II/BCEComparisonTest.png">

Comparing the predictions of each neural network makes the performance differences more clear. In the prediction comparison below, it is clear that while all the models seem to predict motion relatively well, the hierarchy is clearly encoding and predicting shape better than the other two models. In the image comparison below, for example, the hierarchy is clearly encoding the  rectangle shape much better than the other neural nets. Just about every visual comparison shows similar results (see Data/PartII for more results). The single PCUs seem only able to generate elongated ovals with fuzzy boundaries, will the hierarchy is able to generate more rectangular shapes with sharper edges. See 'Data/Part II' for more images.

<img src="/Data/Part II/predCompare6.png">


## 5. Discussion
The hierarchy of linear PCUs worked quite well. It generated nice rectangular shapes with sharp edges comparable in quality to that of the convolutional autoencoder in part I, but it predicted motion better and had, on average, a lower BCE than the convolutional autoencoder used in part I. The hierarchy also had the lowest avg BCE across the 500 test videos among all neural nets considered in part I and part II. This is despite it having fewer layers, connections, and more compressed representation of the image than some of the neural nets in part II. 

Why did the PCU hierarchy perform better than the single PCUs of similar size? There may be several contributing factors. I suspect the main factor has to do with the fact that compressed models of the input are made at multiple levels in the hierarchy. At the first level of the hierarchy compressed representations of 1/4 image patches are made, then a more compressed representation of the entire image is made at the second level. The hierarchy is able to use all of these representations to generate the image. The second level PCU uses its representation of the entire image to pass information down to lower level units about what to generate at the next time step. These lower level units can then use their less compressed, more detailed representations to generate an image patch that is more detailed. Compare this to a single PCU unit. This unit only has one hidden layer/representation it can use to generate an image, which is its smallest layer. There is just something about the hierarchy's ability to create and utilize multiple compressed models at multiple levels of detail that aid it in making predictions.

This sort of hierarchy is easily scalable too. Piekniewski points out the scalability of these hierarchies, and builds a large hierarchy that can track objects in video quite well. Clearly there is something to be said for this kind of architecture.

There is much more to explore in building these architectures that was not explored here. First, incorporating convolutation into the hierarchy may be useful, especially in early stages. For example, convolutional layers could be used to seperate the image into seperate channels, each of which is processed by its own hierarchy. This would not be unlike the way the visual cortex is seperated into somewhat seperate areas all processing different kinds of information. Or a single hierarchy could simply take as input an image that is preprocessed and split into many channels by convolutional layers. Second, hierarchies with different dimensions could also be explored. Third, lateral signals could also be incorporated into the network, and attention could be added. There are many possible ways these hierarchies could be tweaked and added to be more neuromorphic and to perform better. I'll surely be thinking about how to do this in the future.



## Works Cited

Huang, Y., & Rao, R. P. (2011). Predictive coding. Wiley Interdisciplinary Reviews: Cognitive Science, 2(5), 580-593.

Lotter, W., Kreiman, G., & Cox, D. (2016). Deep predictive coding networks for video prediction and unsupervised learning. arXiv preprint arXiv:1605.08104.

Piekniewski, F., Laurent, P., Petre, C., Richert, M., Fisher, D., & Hylton, T. (2016). Unsupervised learning from continuous video in a scalable predictive recurrent network. arXiv preprint arXiv:1607.06854.

Rao, R. P. N. and Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some
extra-classical receptive-field effects. Nature Neuroscience, 2(1):79–87.

Spratling, M. W. (2017). A review of predictive coding algorithms. Brain and cognition, 112, 92-97.
