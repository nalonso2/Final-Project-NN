# Simple Predictive Coding Networks for Representing Shape and Motion
### By Nick Alonso

## 1. Introduction
  1. intro to topic
  2. intro to predictive coding.
  3. My goal for this project was to 1. develop a better understanding of how  predictive coding models work (with a special focus on Rao and Ballard (1999)) and 2. to implement some simple predictive coding networks using pytorch. My focus, therefore, was to learn and implement a working predictive coding architecture rather than to build a network for the purpose of meeting or exceeding some performance benchmark. For this reason, I decided to build a larger number of small neural networks that trained quickly on somewhat simple data, rather than one or a few large networks on more complex data. This allowed me to experiment with various predictive coding architectures quickly, and understand how they worked and what might improve (or worsen) their performance.
  4. 
  
## 2. Data: Video Generation
  Rao and Ballard (1999) built a predictive coding unit to generate an image, but the same unit could also be used to predict video. In the case of video prediction, the unit would not simply be trained to generate (at time t1) an image (presented at prior timestep t0), but rather to predict what new image of the video will appear (at time t1) given the image that appeared at a prior timestep (time t0). In order to predict video, the unit has to learn regularities in the way images change over time rather than just the spatial properties of the image. Predictive coding units have been applied by others for the purpose of video prediction with some success (e.g. .....).
  
  I decided to use video as my input for my networks. My neural networks were going to be small, so the videos used as input could not be too complex. I decided generating simple video would be the easiest way to get this data. I created videos of black rectangles moving across a white background. The size of the background could be adjusted, but I used a small 30x30 frame for all of my tests. Each rectangle was composed of roughly the same number of pixels (12 to 16), but had one of five different shapes: 4x4, 3x5, 5x3, 2x8, 8x2. Each rectangle moved either straight up or down, straight left or right, or at a 45 degree angle up-left, up-right, down-left, or down-right. They also moved at various speeds (usually 1-4 pixels per frame). Thus, the neural networks, in order to generate the videos accurately, had to learn to encode information about shape and motion (i.e. direction and speed).
  
  Each video started with a rectangle positioned randomly somewhere around the center, then proceeded to move until it left the screen. Each video was a 4D pytorch tensor, (batch, frameHeight, frameWidth, frameNumber). These were loaded into a dictionary. Using a dictionary allowed me to store videos of varying length in the same data structure. This would not be possible if I had instead padded the videos to all be the same length and stored them in a tensor. I tried the padding technique, but it affecting the results (likely because many videos were quite short and needed a lot of padding). So I decided to not use padding, but instead store the videos in a dictionary without padding.

## 3. PartI: Three Simple Predictive Coding Units
  The first part of my project consisted of building and experimenting with single predictive coding units with various kinds of layers, activation functions, and size. I present three of these units here. Each predictive coding unit can be seen as special kind of autoencoder. It is important, first, to keep in mind the predictive coding unit is not just generating an image, but is predicting a future image in a video. Second, the encoder of the PCU does not take in an image as input, but rather takes in an error signal as input, which I will explain below. Third, predictive coding units can be combined into a hiearchy of units and take in input from units higher-up in the hierarchy. I focus on building hiearchy in the next section. Here I will only focus on building a single predictive coding unit.
  
  An error signal can be understood as follows. Let's call the image at timestep t0 I<sub>0</sub> and the prediction of that image generated by a predictive coding unit (PCU) I<sub>Pred0</sub>. The error signal that the PCU will take as input to predict the next frame at time t1 is some measure the different between I<sub>0</sub> and I<sub>Pred0</sub>. Rao and Ballard used the simple measure I<sub>0</sub> - I<sub>0</sub>, where the subtraction is an element-wise subtraction of the two matrices. The result is a error signal which is then inputted to the PCU. Other measures of the error may be used. Spratling (2017), for example, suggests that a*I<sub>0</sub> / b*I<sub>0</sub> + c is more biologically plausible (a,b, and c are constants). (Though I don't show results, I found both errors acheived similar results so I stuck with Rao and Ballards simpler error computation). Rao and Ballard (1999, 2011) argue the error signal is a more efficient way to encode and feedforward information, as only information about the unpredicted/unexpected aspects of the input are propogated forward rather than all of it, which, on average, reduced feedforward activity.
  
I experimented with many different kinds of PCU. Here I present three simple ones. First, is a PCU that consists of two linear layers (not counting the input layer), a one layer encoder and a one layer decoder. The input and output layer are size 1x900. The hidden layer is of size 1x400. The encoder used ReLU activation functions, while the decoder used sigmoid, as its ouput must be between 0 and 1.

Second, is a  PCU that used convolutional layers. I consisted of two layers (not including input layer), 1 convolutional layer encoder and one convolutional layer decoder. Input and output layers were 1x30x30. A kernal of size 4 and a stride of two were ysed, with 1 to 3 output channels. Here I show the results of a PCU with an encoder that has 2 output channels. 

Third, I created a variational autoencoder. I used code found here https://github.com/pytorch/examples/blob/master/vae/main.py to build the network. This code is an implementation of Kingma and Wellington (2014). This network had an input layer of size 1x900, which fed into a hidden layer h1 of 1x400. H1 fed into two more hidden layers of size 1x120. These encoded mu and var variables (see code). Mu and var are used for reparameterization, produces a vector of size 1x120. This is fed into the decoder which consists of three linear layers size, 1x120, 1x400, and 1x900.

Each network took an error signal as input. The error signal was, as noted above, I<sub>0</sub> and I<sub>Pred0</sub>. They were trained using backpropigation. Here I used BCE loss to train the linear and convolutional PCU units. (Rao and Ballard (1999) use a different optimization function but I do not use that here). The loss function for the variational autoencoder was more complex, and I will refer you to the code (found in finalProjVAE.py) for more details. The avg loss per video during the training of each of these PCUs can be found in the data folder (see........). 

Each network performed slightly differently. Because not all the networks use the same loss function I do not compare their losses here, but much can be learned by looking at the images the generate. Below you will see a side by side comparison of video frames and the predictions of those frames produces by each PCU. Each network predicts random noise on the first frame. Then they all roughly generate the previous frame. One frame is not enough to know what direct and speed these shapes are moving yet, so they will need one or more frames to figure out this information.

It becomes clear that both the VAE and the linear PCU predict motion, as they eventually stop generating the previous frame and begin predicting that the shape will be in a different position than it was last frame (i.e. that it will move). The VAE, however, seems better than the simple linear unit at encoding shape. The convolutional encoders generates shape well, but never seems to encode information about motion. It always seems to generate the previous frame.

## 4. PartII: Hierarchy of Predictive Coding Units




## 5. Conclusion
